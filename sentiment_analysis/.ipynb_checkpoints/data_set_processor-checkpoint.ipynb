{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd103658-7c25-45ed-a3f6-34a79c80e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620e225-c3d0-463a-9e11-2b6e8a98e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from datasets\n",
    "\n",
    "data_categories = ['books', 'dvd', 'electronics', 'kitchen_&_housewares']\n",
    "review_categories = ['positive', 'negative', 'unlabeled']\n",
    "\n",
    "data_dir = \"sorted_data_acl\"\n",
    "\n",
    "# load data\n",
    "def review_data_load():\n",
    "    data_map = {data_category: {} for data_category in data_categories}\n",
    "    \n",
    "    for data_category in data_categories:\n",
    "        for review_category in review_categories:\n",
    "            file_path = os.path.join(data_dir, data_category, f\"{review_category}.review\")\n",
    "            \n",
    "            if os.path.exists(file_path):               \n",
    "                try:\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        data_map[data_category][review_category] = file.read()\n",
    "                except UnicodeDecodeError as e:\n",
    "                    with gzip.open(file_path, 'rt', encoding='utf-8') as file:\n",
    "                        data_map[data_category][review_category] = file.read()\n",
    "            else:\n",
    "                data_map[data_category][review_category] = None\n",
    "    \n",
    "    return data_map\n",
    "\n",
    "data_map = review_data_load()\n",
    "\n",
    "#check data\n",
    "def check_loaded_data():\n",
    "    for data_category, review_categories_data in data_map.items():\n",
    "        for review, review_category_data in review_categories_data.items():\n",
    "            print(f\"{data_category} has {review} type {type(review_category_data)} size {len(review_category_data) if review_category_data else 0}\")\n",
    "\n",
    "check_loaded_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159d16b-4f25-4fcd-8f70-a427d4ec25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xml nodes from files to data structures\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "from fractions import Fraction\n",
    "from enum import Enum\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, rating, review_text, helpfull, title):\n",
    "        self.rating = rating\n",
    "        self.review_text = review_text\n",
    "        self.title = title\n",
    "\n",
    "        if helpfull is None or \" of \" not in helpfull:\n",
    "            self.helpfull = None\n",
    "        else:\n",
    "            numerator, denominator = helpfull.split(\" of \")\n",
    "            numerator = int(numerator)\n",
    "            denominator = int(denominator)\n",
    "            self.helpfull = Fraction(int(numerator), int(denominator))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Review(rating={self.rating}, helpfull={self.helpfull if self.helpfull is not None else \"\"}, title={self.title}, review_text={self.review_text})\"\n",
    "\n",
    "def wrap_with_root(xml_string):\n",
    "    return f\"<root>{xml_string}</root>\"\n",
    "\n",
    "def parse_review_XML_text_to_structures(reviews_text):\n",
    "    wrapped_text = wrap_with_root(reviews_text)\n",
    "    try:\n",
    "        root = ET.fromstring(wrapped_text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(wrapped_text[57162753:57162793])\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    print(f\"Number of child elements: {len(root)}\")\n",
    "    \n",
    "    for review in root.findall('review'):\n",
    "        helpful = review.find('helpful').text\n",
    "        rating = int(float(review.find('rating').text))\n",
    "        title = review.find('title').text\n",
    "        review_text = review.find('review_text').text\n",
    "\n",
    "        reviews.append(Review(rating, review_text, helpful, title))\n",
    "\n",
    "    print(f\"It {len(reviews_text)} parsed to {len(reviews)} reviews\")\n",
    "    return reviews\n",
    "\n",
    "def parse_data_to_reviews(data_map):\n",
    "    reviews_map = {}\n",
    "    \n",
    "    for data_category, review_categories_data in data_map.items():\n",
    "        for review_category, review_category_data in review_categories_data.items():\n",
    "            print(f\"Parse {review_category} {data_category}\")\n",
    "            review_category_data = re.sub(r'[\\x00-\\x1F\\x7F]', '', review_category_data)\n",
    "\n",
    "            outer = reviews_map.get(data_category, {})\n",
    "            outer[review_category] = parse_review_XML_text_to_structures(review_category_data.replace('&', '&amp;'))\n",
    "            reviews_map[data_category] = outer\n",
    "    \n",
    "    return reviews_map\n",
    "\n",
    "reviews_map = parse_data_to_reviews(data_map)\n",
    "\n",
    "#check data\n",
    "def check_parsed_data():\n",
    "    for data_category, review_categories_data in reviews_map.items():\n",
    "        for review_category, review_category_data in review_categories_data.items():\n",
    "            print(f\"{data_category} has {review_category} type {type(review_category_data)} size {len(review_category_data)}\")\n",
    "            print(review_category_data[:2])\n",
    "            print(review_category_data[-2:])\n",
    "\n",
    "check_parsed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c741719-ecb4-4c0d-8bcd-6b7d35191988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alex937/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alex937/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books positive started\n",
      "books positive finished\n",
      "books negative started\n",
      "books negative finished\n",
      "books unlabeled started\n",
      "books unlabeled finished\n",
      "dvd positive started\n",
      "dvd positive finished\n",
      "dvd negative started\n",
      "dvd negative finished\n",
      "dvd unlabeled started\n",
      "dvd unlabeled finished\n",
      "electronics positive started\n",
      "electronics positive finished\n",
      "electronics negative started\n",
      "electronics negative finished\n",
      "electronics unlabeled started\n",
      "electronics unlabeled finished\n",
      "kitchen_&_housewares positive started\n",
      "kitchen_&_housewares positive finished\n",
      "kitchen_&_housewares negative started\n",
      "kitchen_&_housewares negative finished\n",
      "kitchen_&_housewares unlabeled started\n",
      "kitchen_&_housewares unlabeled finished\n"
     ]
    }
   ],
   "source": [
    "# Check spells, remove special characters, expanding contractions etc...\n",
    "import contractions\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# pip install spellchecker, pyspellchecker\n",
    "# python3 -m nltk.downloader all\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.x')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "spell = SpellChecker()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "to_remove_punctuation = True\n",
    "to_convert_to_lowercase = True\n",
    "to_remove_numbers = False\n",
    "to_remove_extra_whitespace = True\n",
    "to_correct_spelling = True\n",
    "to_remove_stop_words = True\n",
    "to_stem_words = False\n",
    "to_lemmatize_words = True\n",
    "to_remove_specs = True\n",
    "to_expand_contractions = True\n",
    "\n",
    "# Punctuation often doesn't add much meaning for many NLP tasks and can be removed.\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "#Converting all text to lowercase ensures that words are treated uniformly.\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# In some cases, numbers might not be relevant.\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Extra whitespace and newline characters should be removed or normalized.\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Stop words (common words that donâ€™t add much meaning) can be removed.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def correct_spelling(text): # need to improve performance\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in words:\n",
    "        correct_word = spell.candidates(word)\n",
    "        corrected_words.append(word if correct_word is None else list(correct_word)[0])\n",
    "    \n",
    "    #print(corrected_words)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "def correct_spelling2(text): # need to improve performance\n",
    "    b = TextBlob(text)\n",
    "    return b.correct()\n",
    "\n",
    "# Stemming reduces words to their root form.\n",
    "def stem_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Lemmatization reduces words to their base or dictionary form, usually providing better results than stemming.\n",
    "def lemmatize_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "# remove emails\n",
    "def remove_emails(text):\n",
    "    return re.sub(r'\\b[\\w.%+-]+@[\\w.-]+\\.[a-zA-Z]{2,6}\\b', '', text)\n",
    "\n",
    "# remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_specs(text):\n",
    "    return remove_special_characters(remove_emails(remove_urls(text)))\n",
    "\n",
    "# expanding contractions to full forms for more consistent analysis\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "clean_function_list = [\n",
    "    (to_remove_extra_whitespace, remove_extra_whitespace),\n",
    "    (to_remove_punctuation, remove_punctuation),\n",
    "    (to_correct_spelling, correct_spelling2),\n",
    "    (to_expand_contractions, expand_contractions),\n",
    "    (to_remove_stop_words, remove_stop_words),\n",
    "    (to_stem_words, stem_words),\n",
    "    (to_lemmatize_words, lemmatize_words),\n",
    "    (to_convert_to_lowercase, convert_to_lowercase),\n",
    "    (to_remove_numbers, remove_numbers),\n",
    "    (to_remove_specs, remove_specs)\n",
    "]\n",
    "\n",
    "def clean_data(text):\n",
    "    for to_do_clean, clean_func in clean_function_list:\n",
    "        if not text:\n",
    "            return ''\n",
    "        if to_do_clean:\n",
    "            text = clean_func(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "reviews_map_cleaned = reviews_map.copy()\n",
    "\n",
    "def clean_reviews():\n",
    "    for data_category, review_categories_data in reviews_map_cleaned.items():\n",
    "        for review_category, review_category_data in review_categories_data.items():\n",
    "            print(f\"{data_category} {review_category} started\")\n",
    "            for review in review_category_data:\n",
    "                review.review_text = clean_data(review.review_text)\n",
    "                review.title = clean_data(review.title)\n",
    "            print(f\"{data_category} {review_category} finished\")\n",
    "\n",
    "clean_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6765d19-1fac-4ae1-8a76-9762eaa0cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clean_data():\n",
    "    for data_category, review_categories_data in reviews_map_cleaned.items():\n",
    "        for review_category, review_category_data in review_categories_data.items():\n",
    "            print(f\"{data_category} has {review_category} type {type(review_category_data)} size {len(review_category_data)}\")\n",
    "            print(review_category_data[:2])\n",
    "            print(review_category_data[-2:])\n",
    "\n",
    "check_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6e2d5c53-037d-454f-83ce-2ff48fade8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/alex937/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# prepare Pandas dataset\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "valid_words = set(words.words())\n",
    "\n",
    "# to check if a review text has a sufficient number of valid words\n",
    "def is_meaningful_review(review_text, min_valid_words=5):\n",
    "    word_list = review_text.split()\n",
    "    valid_word_count = sum(1 for word in word_list if word in valid_words)\n",
    "    return valid_word_count >= min_valid_words  \n",
    "\n",
    "# to estimate review\n",
    "class ReviewCategory(Enum):\n",
    "    POS = 1\n",
    "    NEG = 2\n",
    "    UNDEF = 3\n",
    "    \n",
    "def mark_review(review, review_category):\n",
    "    if review_category == 'positive':\n",
    "        return ReviewCategory.POS\n",
    "    if review_category == 'negative':\n",
    "        return ReviewCategory.NEG\n",
    "\n",
    "    if review.rating > 3:\n",
    "        return ReviewCategory.POS\n",
    "    if review.rating < 3:\n",
    "        return ReviewCategory.NEG\n",
    "    \n",
    "    return ReviewCategory.UNDEF\n",
    "\n",
    "def prepare_data_to_pandas():\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    # to removes duplicate reviews \n",
    "    seen = set() \n",
    "\n",
    "    for data_category, review_categories_data in reviews_map_cleaned.items():\n",
    "        for review_category, review_category_data in review_categories_data.items():\n",
    "            for review in review_category_data:\n",
    "                if not review.title:\n",
    "                    if not review.review_text:\n",
    "                        continue\n",
    "                    full_review_text = review.review_text\n",
    "                if not review.review_text:\n",
    "                    full_review_text = review.title\n",
    "                else:\n",
    "                    full_review_text = f\"{review.title} {review.review_text}\"\n",
    "                \n",
    "                # check correctnesses\n",
    "                if is_meaningful_review(full_review_text) == False:\n",
    "                    continue\n",
    "                \n",
    "                # check duplicates\n",
    "                if full_review_text in seen:\n",
    "                    continue\n",
    "                else:\n",
    "                    seen.add(full_review_text)\n",
    "    \n",
    "                # check clean estimating\n",
    "                review_mark = mark_review(review, review_category)\n",
    "                if(review_mark == ReviewCategory.UNDEF):\n",
    "                    continue\n",
    "                \n",
    "                texts.append(full_review_text)\n",
    "                labels.append(1 if review_mark == ReviewCategory.POS else 0)\n",
    "    \n",
    "    return {'text': texts, 'label': labels}\n",
    "\n",
    "data_to_pandas = prepare_data_to_pandas()\n",
    "df = pd.DataFrame(data_to_pandas)\n",
    "\n",
    "df.to_csv('reviews_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e3f82-30b8-4ee5-9ae6-1155387f394b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
