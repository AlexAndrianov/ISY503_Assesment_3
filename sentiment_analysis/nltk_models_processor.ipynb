{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e65f11c7-0a14-4558-adfe-b2966702b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57619ff4-398d-4f5a-8442-c13179cc3e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  one best crichton novel sphere michael crichto...      1\n",
      "1  medicine future z accomplished heart surgeon f...      1\n",
      "2  beautiful gorgeous network comic book contains...      1\n",
      "3  lover robicheaux book lover robicheaux demon s...      1\n",
      "4  excellent broad survey development civilizatio...      1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reviews_dataset2.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe46560c-eab0-4a79-a27a-a78a9f5d6cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex937/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# loading prepared dataset and splitting it to train and test parts\n",
    "nltk.download('punkt')\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "def to_features(words):\n",
    "    return {word: True for word in words}\n",
    "\n",
    "dataset = [(to_features(word_tokenizer(text)), label) for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b208cc3-c3e8-4179-b387-1bf67f3a85f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Most Informative Features\n",
      "                  refund = True                0 : 1      =     32.8 : 1.0\n",
      "           directtovideo = True                0 : 1      =     18.6 : 1.0\n",
      "               gibberish = True                0 : 1      =     18.6 : 1.0\n",
      "             backordered = True                0 : 1      =     18.0 : 1.0\n",
      "               excusable = True                0 : 1      =     18.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# NaiveBayesClassifier with direct usging Pandas DataFrame\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(f'Accuracy: {accuracy(classifier, test_data):.2f}')\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17dc0389-a8c0-4387-968e-ca13706e6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70481\n",
      "17620\n",
      "[({'one': True, 'best': True, 'crichton': True, 'novel': True, 'sphere': True, 'michael': True, 'excellant': True, 'certainly': True, 'hardest': True, 'put': True, 'read': True, 'story': True, 'revolves': True, 'around': True, 'man': True, 'named': True, 'norman': True, 'johnson': True, 'phycologist': True, 'travel': True, '4': True, 'civilans': True, 'remote': True, 'location': True, 'pacific': True, 'ocean': True, 'help': True, 'navy': True, 'top': True, 'secret': True, 'misssion': True, 'quickly': True, 'learn': True, 'half': True, 'mile': True, 'long': True, 'spaceship': True, 'center': True, '1000': True, 'foot': True, 'live': True, 'researching': True, 'spacecraft': True, 'joined': True, '5': True, 'personel': True, 'run': True, 'operation': True, 'however': True, 'surface': True, 'typhoon': True, 'come': True, 'support': True, 'ship': True, 'must': True, 'leave': True, 'team': True, 'ten': True, 'stuck': True, 'day': True, 'sea': True, 'find': True, 'actually': True, 'american': True, 'explored': True, 'black': True, 'hole': True, 'brought': True, 'back': True, 'strange': True, 'thing': True, 'earththis': True, 'research': True, 'still': True, 'lot': True, 'information': True, 'random': True, 'lawes': True, 'partial': True, 'pressure': True, 'behavior': True, 'analysisi': True, 'would': True, 'strongly': True, 'recommend': True, 'book': True}, 1), ({'medicine': True, 'future': True, 'dr': True, 'oz': True, 'accomplished': True, 'heart': True, 'surgeon': True, 'field': True, 'cardiac': True, 'transplantation': True, 'describes': True, 'combine': True, 'complementary': True, 'eg': True, 'hypnosis': True, 'reflexology': True, 'yoga': True, 'message': True, 'acupuncture': True, 'etc': True, 'orthodox': True, 'western': True, 'excellent': True, 'forward': True, 'dean': True, 'ornish': True, 'interesting': True, 'epilogue': True, 'containing': True, 'overview': True, 'technique': True, 'bulk': True, 'book': True, 'contains': True, 'story': True, 'patient': True, 'treated': True, 'using': True, 'revolutionary': True, 'way': True, 'cardiologist': True, 'great': True, 'interest': True, 'combining': True, 'reason': True, 'bought': True, 'however': True, 'bit': True, 'boring': True, 'read': True, 'also': True, 'disappointment': True, 'nevertheless': True, 'interested': True, 'new': True, 'think': True, 'millennium': True, 'want': True}, 1), ({'beautiful': True, 'gorgeous': True, 'artwork': True, 'comic': True, 'book': True, 'contains': True, 'extraordinary': True, 'alex': True, 'ross': True, 'superman': True, 'batman': True, 'wonder': True, 'woman': True, 'justice': True, 'league': True, 'etc': True, 'even': True, 'hannabarbera': True, 'fan': True, 'recieved': True, 'christmas': True, 'gift': True, 'read': True, 'againa': True, 'musthave': True}, 1)]\n"
     ]
    }
   ],
   "source": [
    "# decrease size of dataset to reduce train and test times\n",
    "print(len(dataset))\n",
    "size_short_dataset = int(len(dataset)/4)\n",
    "short_dataset = dataset[:size_short_dataset]\n",
    "print(len(short_dataset))\n",
    "print(short_dataset[:3])\n",
    "\n",
    "short_train_data, short_test_data = train_test_split(short_dataset, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c1c2ee-9d9b-4706-b55b-69a5d24ad25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tokenizing\n",
      "Start splitting\n",
      "Start classifying\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# NaiveBayesClassifier with direct usging Pandas DataFrame and bigrams tokenizer\n",
    "from nltk import bigrams\n",
    "\n",
    "def tokenize_with_bigrams(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    bigram_features = list(bigrams(tokens))\n",
    "    all_features = tokens + [\"_\".join(bigram) for bigram in bigram_features]\n",
    "    return {word: True for word in all_features}\n",
    "\n",
    "print('Start tokenizing')\n",
    "dataset_bigrams = [(tokenize_with_bigrams(text), label) for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "print('Start splitting')\n",
    "train_data_bg, test_data_bg = train_test_split(dataset_bigrams, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Start classifying')\n",
    "classifier_bg = NaiveBayesClassifier.train(train_data_bg)\n",
    "\n",
    "print(f'Accuracy: {accuracy(classifier_bg, test_data_bg):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87457b7-bc16-4633-8fd0-afd89c21c209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tokenizing\n",
      "Start splitting\n",
      "Start classifying\n",
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "# only bigrams\n",
    "from nltk import bigrams\n",
    "\n",
    "def tokenize_with_bigrams(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    bigram_features = list(bigrams(tokens))\n",
    "    all_features = [\"_\".join(bigram) for bigram in bigram_features]\n",
    "    return {word: True for word in all_features}\n",
    "\n",
    "print('Start tokenizing')\n",
    "dataset_bigrams = [(tokenize_with_bigrams(text), label) for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "print('Start splitting')\n",
    "train_data_bg, test_data_bg = train_test_split(dataset_bigrams, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Start classifying')\n",
    "classifier_bg = NaiveBayesClassifier.train(train_data_bg)\n",
    "\n",
    "print(f'Accuracy: {accuracy(classifier_bg, test_data_bg):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4970923d-2149-4e94-a82a-5b45e9d51c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tokenizing\n",
      "Start splitting\n",
      "Start classifying\n",
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# ngramms\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def tokenize_with_ngrams(text, n=5):\n",
    "    tokens = word_tokenize(text)\n",
    "    ngram_features = list(ngrams(tokens, n))\n",
    "    all_features = tokens + [\"_\".join(ngram) for ngram in ngram_features]\n",
    "    return {word: True for word in all_features}\n",
    "\n",
    "print('Start tokenizing')\n",
    "dataset_ngrams = [(tokenize_with_ngrams(text), label) for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "print('Start splitting')\n",
    "train_data_ng, test_data_ng = train_test_split(dataset_ngrams, test_size=0.3, random_state=42)\n",
    "\n",
    "print('Start classifying')\n",
    "classifier_ng = NaiveBayesClassifier.train(train_data_ng)\n",
    "\n",
    "print(f'Accuracy: {accuracy(classifier_ng, test_data_ng):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f36bcbfa-7535-4e6e-ad1c-58a6303a7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model to file\n",
    "import pickle\n",
    "\n",
    "with open('naive_bayes_model_with_simple_tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(classifier, file)\n",
    "\n",
    "#with open('naive_bayes_model_with_5_gramm_tokenizer.pkl', 'wb') as file:\n",
    "#    pickle.dump(classifier_ng, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b31b0-92ff-4f93-a01e-2337674da30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaiveBayesClassifier with PyTorch DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe['text'].values\n",
    "        self.labels = dataframe['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return text, label\n",
    "\n",
    "dataset = ReviewsDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "train_data = []\n",
    "for texts, labels in dataloader:\n",
    "    for text, label in zip(texts, labels):\n",
    "        features = to_features(word_tokenizer(text))\n",
    "        train_data.append((features, label))\n",
    "\n",
    "# Обучение модели\n",
    "classifier_2 = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(f'Accuracy: {accuracy(classifier_2, test_data):.2f}')\n",
    "classifier_2.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc60f3d8-f28e-4c85-9ad2-85659086da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "# using NLTK + Custom Feature Extraction\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "# nltk.download('opinion_lexicon')\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "def to_features_2(words):\n",
    "    features = {}\n",
    "    features['number_of_positive_features'] = len([word for word in words if word in positive_words])\n",
    "    features['number_of_negative_features'] = len([word for word in words if word in negative_words])\n",
    "    return features\n",
    "\n",
    "all_dataset = [(to_features_2(word_tokenizer(text)), label) for text, label in zip(df['text'], df['label'])]\n",
    "\n",
    "# filter wrong datasets to improve accuracy\n",
    "filtered_train_dataset = [t for t in all_dataset if (t[1] == 1 and t[0]['number_of_positive_features'] > t[0]['number_of_negative_features']) \n",
    "                          or (t[1] == 0 and t[0]['number_of_positive_features'] < t[0]['number_of_negative_features'])]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_3_all, test_data_3_all = train_test_split(all_dataset, test_size=0.3, random_state=42)\n",
    "train_data_3_filtered, test_data_3_filtered = train_test_split(filtered_train_dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "classifier_3 = NaiveBayesClassifier.train(train_data_3_filtered)\n",
    "print(f'Accuracy: {accuracy(classifier_3, test_data_3_all):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb722e-282d-4275-9877-12b2dcd503d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DecisionTreeClassifier with direct usging Pandas DataFrame\n",
    "# very slow method\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "classifier_4 = DecisionTreeClassifier.train(short_train_data)\n",
    "print(f'Accuracy: {accuracy(classifier_4, short_test_data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ccb26c5-1e59-4465-b77f-a02eada49bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (20 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex937/.local/lib/python3.12/site-packages/nltk/classify/maxent.py:1380: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2**nf_delta\n",
      "/home/alex937/.local/lib/python3.12/site-packages/nltk/classify/maxent.py:1381: RuntimeWarning: overflow encountered in multiply\n",
      "  nf_exp_nf_delta = nftranspose * exp_nf_delta\n",
      "/home/alex937/.local/lib/python3.12/site-packages/nltk/classify/maxent.py:1382: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/home/alex937/.local/lib/python3.12/site-packages/nltk/classify/maxent.py:1383: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.188\n",
      "Accuracy: 0.19\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with direct usging Pandas DataFrame\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "classifier_5 = MaxentClassifier.train(train_data, max_iter=20)\n",
    "print(f'Accuracy: {accuracy(classifier_5, test_data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e076b9-6e32-4836-b19f-167a996d4a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n",
      "         Final               nan        0.188\n",
      "Accuracy: 0.19\n"
     ]
    }
   ],
   "source": [
    "# maximum entropy classifier with direct usging Pandas DataFrame\n",
    "from nltk.classify import ConditionalExponentialClassifier\n",
    "\n",
    "classifier_6 = ConditionalExponentialClassifier.train(train_data)\n",
    "print(f'Accuracy: {accuracy(classifier_6, test_data):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e4f0bcf-aa11-418b-9023-4c969766faa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number os dataset elements is: 70481\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# using opinion_lexicon without classifier\n",
    "import random\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "def to_features_2(words):\n",
    "    features = {}\n",
    "    features['number_of_positive_features'] = len([word for word in words if word in positive_words])\n",
    "    features['number_of_negative_features'] = len([word for word in words if word in negative_words])\n",
    "    return features\n",
    "\n",
    "dataset_text_label = [(text, label) for text, label in zip(df['text'], df['label'])]\n",
    "print(f'Number os dataset elements is: {len(dataset_text_label)}')\n",
    "\n",
    "random.shuffle(dataset_text_label)\n",
    "\n",
    "#print(dataset_text_label[:5])\n",
    "\n",
    "def classify_text_by_opinion_lexicon(text):\n",
    "    features = to_features_2(word_tokenize(text))\n",
    "    return 1 if features['number_of_positive_features'] >= features['number_of_negative_features'] else 0\n",
    "\n",
    "def calculate_accuracy(texts_set):\n",
    "    correct = 0\n",
    "    iterations_number = 0\n",
    "    for text, label in texts_set:\n",
    "        iterations_number += 1\n",
    "        prediction = classify_text_by_opinion_lexicon(text)\n",
    "        if prediction == label:\n",
    "            correct += 1\n",
    "        if iterations_number == 1000:\n",
    "            iterations_number = 0\n",
    "            #print(f\"Next thousand was passed correct accuracy is {correct}\")\n",
    "    \n",
    "    return correct\n",
    "\n",
    "accuracy = calculate_accuracy(dataset_text_label) / len(dataset_text_label)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d461469-9b0f-4f8c-b17a-ada36ba622cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
