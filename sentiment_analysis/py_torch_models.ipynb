{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7bb22-5cee-4020-8064-bbd9e533dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('reviews_dataset2.csv')\n",
    "print(df.head())\n",
    "\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(shuffled_df.head())\n",
    "\n",
    "texts = shuffled_df['text'].values\n",
    "labels = shuffled_df['label'].values\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts[:10000]).toarray()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(labels[:10000])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(y_train[:3])\n",
    "print(X[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d513490-fd02-4a96-ba15-2e4f3c636e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple neuron network\n",
    "# near 85 % accuracy only\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# create\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "model = SimpleNN(input_dim=x_train.shape[1], hidden_dim=1000, output_dim=len(set(Y)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    inputs = torch.tensor(x_train, dtype=torch.float32)\n",
    "    targets = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# estimating\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = torch.tensor(x_test, dtype=torch.float32)\n",
    "    targets = torch.tensor(y_test, dtype=torch.long)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == targets).float().mean()\n",
    "    print(f'Test Accuracy: {accuracy.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638cf12-1959-4711-93d4-e9cfc6afa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# near 80 % accuracy only...\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('reviews_dataset2.csv')\n",
    "print(df.head())\n",
    "\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(shuffled_df.head())\n",
    "\n",
    "texts = shuffled_df['text'].values\n",
    "labels = shuffled_df['label'].values\n",
    "\n",
    "# tokenize and build vocabulary\n",
    "vocab = Counter(\" \".join(texts).split())\n",
    "\n",
    "# words to indexes mapping\n",
    "vocab = {word: i for i, (word, freq) in enumerate(vocab.items(), 1)}  \n",
    "\n",
    "# convert texts to sequences of numbers\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab[word] for word in text.split() if word in vocab]\n",
    "\n",
    "sequences = [text_to_sequence(text, vocab) for text in texts]\n",
    "\n",
    "# setting sequences to have the same length\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = [seq + [0] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "inputs = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# split into train and test sets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, labels, test_size=0.3)\n",
    "\n",
    "# model\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * max_len, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        flattened = embedded.view(embedded.size(0), -1)\n",
    "        out = torch.relu(self.fc1(flattened))\n",
    "        out = self.sigmoid(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(vocab) + 1  # plus one for padding (index 0)\n",
    "embedding_dim = 27\n",
    "hidden_dim = 17\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "model = SentimentAnalysisModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(train_inputs)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs)\n",
    "    predictions = (test_outputs >= 0.5).float()  # Convert probabilities to binary predictions\n",
    "    accuracy = (predictions == test_labels).sum() / len(test_labels)\n",
    "    print(f'Accuracy: {accuracy.item() * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e6284037-cb3b-46e1-b217-0cf72c55d428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  one best crichton novel sphere michael crichto...      1\n",
      "1  medicine future z accomplished heart surgeon f...      1\n",
      "2  beautiful gorgeous network comic book contains...      1\n",
      "3  lover robicheaux book lover robicheaux demon s...      1\n",
      "4  excellent broad survey development civilizatio...      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex937/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547bd25ec9444decb7abbb6349c2849e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ca706ea06946ddadf838da108a19f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lengths: 1024 1024 1024\n",
      "Epoch 1/6, Train Loss: 0.4592, Valid Loss: 0.3512, Accuracy: 84.05%\n",
      "Epoch 2/6, Train Loss: 0.2843, Valid Loss: 0.2676, Accuracy: 89.48%\n",
      "Epoch 3/6, Train Loss: 0.2191, Valid Loss: 0.2484, Accuracy: 90.49%\n",
      "Epoch 4/6, Train Loss: 0.1864, Valid Loss: 0.2435, Accuracy: 90.92%\n",
      "Epoch 5/6, Train Loss: 0.1652, Valid Loss: 0.2452, Accuracy: 90.99%\n",
      "Epoch 6/6, Train Loss: 0.1478, Valid Loss: 0.2518, Accuracy: 91.04%\n",
      "Model and vocabulary saved to PyTorchNBoWModel.pkl\n"
     ]
    }
   ],
   "source": [
    "# Neural Bag of Words appoach also known as continuous bag-of-words, CBoW\n",
    "# more than 92 % accuracy ! \n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "# pandas to tokens\n",
    "df = pd.read_csv('reviews_dataset2.csv')\n",
    "print(df.head())\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict(train_test_split)\n",
    "\n",
    "train_data = dataset_dict['train']\n",
    "test_data = dataset_dict['test']\n",
    "train_data, test_data\n",
    "\n",
    "train_data.features\n",
    "train_data[0]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# key parameter !\n",
    "max_length=1024\n",
    "\n",
    "def tokenize_text(example):\n",
    "    tokens = tokenizer.tokenize(example[\"text\"], truncation=True, max_length=max_length)\n",
    "    example[\"tokens\"] = tokens\n",
    "    return example\n",
    "\n",
    "# apply the tokenization function\n",
    "train_data = train_data.map(tokenize_text)\n",
    "test_data = test_data.map(tokenize_text)\n",
    "\n",
    "test_size = 0.25\n",
    "\n",
    "# splitting data\n",
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]\n",
    "\n",
    "min_freq = 1 # to more accuracy... :)\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "# building vocabulary\n",
    "def build_vocab_from_iterator(tokens_data, min_freq, specials):\n",
    "    all_tokens = [token for tokens in tokens_data for token in tokens]\n",
    "    token_counts = Counter(all_tokens)\n",
    "    vocab = {token for token, count in token_counts.items() if count >= min_freq}\n",
    "    vocab = specials + sorted(vocab)\n",
    "    vocab_dict = {token: idx for idx, token in enumerate(vocab)}\n",
    "    return vocab_dict\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "\n",
    "# converting tokenized texts to numerical indices\n",
    "def encode_texts_to_indices(texts, vocab):\n",
    "    return [[vocab.get(token, vocab[\"<unk>\"]) for token in tokens] for tokens in texts]\n",
    "\n",
    "# padding sequences to the same length\n",
    "def pad_sequences(sequences, pad_index):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    return ([seq + [pad_index] * (max_len - len(seq)) for seq in sequences], max_len)\n",
    "\n",
    "# converting tokenized texts to numerical indices\n",
    "train_indices = encode_texts_to_indices(train_data[\"tokens\"], vocab)\n",
    "valid_indices = encode_texts_to_indices(valid_data[\"tokens\"], vocab)\n",
    "test_indices = encode_texts_to_indices(test_data[\"tokens\"], vocab)\n",
    "\n",
    "# padding sequences\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "train_indices_padded, max_len_train  = pad_sequences(train_indices, pad_index)\n",
    "valid_indices_padded, max_len_valid = pad_sequences(valid_indices, pad_index)\n",
    "test_indices_padded, max_len_test = pad_sequences(test_indices, pad_index)\n",
    "\n",
    "print(f\"Max lengths: {max_len_train} {max_len_valid} {max_len_test}\")\n",
    "\n",
    "# building tensors\n",
    "X_train = torch.tensor(train_indices_padded, dtype=torch.long)\n",
    "y_train = torch.tensor(train_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "X_valid = torch.tensor(valid_indices_padded, dtype=torch.long)\n",
    "y_valid = torch.tensor(valid_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(test_indices_padded, dtype=torch.long)\n",
    "y_test = torch.tensor(test_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "# instancing data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Neural Bag of Words model\n",
    "class NBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        embedded = self.embedding(ids)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        prediction = self.fc(pooled)\n",
    "        return prediction\n",
    "\n",
    "# defining model parameters\n",
    "embedding_dim = 300\n",
    "output_dim = len(set(df['label']))\n",
    "vocab_size = len(vocab)\n",
    "pad_index = vocab[\"<pad>\"]\n",
    "\n",
    "# initializing model\n",
    "model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training the model\n",
    "def train_model_iteration(model, train_loader, optimizer, criterion):\n",
    "    # train part\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts, labels = batch\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)        \n",
    "    model.eval()\n",
    "    return train_loss\n",
    "\n",
    "# estimating the model\n",
    "def estimate_model_iteration(valid_loader, model, criterion):\n",
    "    valid_loss = 0\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            texts, labels = batch\n",
    "            \n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            corrects += (predicted == labels).sum().item()\n",
    "    \n",
    "    valid_loss /= len(valid_loader)\n",
    "    accuracy = 100 * corrects / total\n",
    "    \n",
    "    return valid_loss, accuracy \n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=6):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_iteration(model, train_loader, optimizer, criterion)        \n",
    "        valid_loss, accuracy = estimate_model_iteration(valid_loader, model, criterion)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer)\n",
    "\n",
    "def save_model_and_vocab(model, vocab, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        dill.dump({'model': model, 'vocab': vocab}, f)\n",
    "    print(f'Model and vocabulary saved to {filename}')\n",
    "\n",
    "model_path = \"PyTorchNBoWModel.pkl\"\n",
    "save_model_and_vocab(model, vocab, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5d93b8f3-5b00-4a9d-99da-6ae6fb004192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex937/.local/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2544, Accuracy: 91.30%\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_vocab(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    model = data['model']\n",
    "    vocab = data['vocab']\n",
    "    model.eval()\n",
    "    return model, vocab\n",
    "\n",
    "# Load the model and vocabulary\n",
    "model, vocab = load_model_and_vocab(model_path)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, accuracy = estimate_model_iteration(test_loader, model, criterion)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c3e8aa67-6fd3-48a8-bbc4-2ac978b81a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label 1: 0\n",
      "Predicted label 2: 1\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, vocab, max_length=1024):\n",
    "    tokens = text.split()\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    \n",
    "    if len(indices) < max_length:\n",
    "        indices += [vocab['<pad>']] * (max_length - len(indices))\n",
    "    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def predict(model, text, vocab):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = preprocess_text(text, vocab)\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "text = \"negative reaction terrible bad disgusting\"\n",
    "text2 =\"good better maybe forced\"\n",
    "predicted_label = predict(model, text, vocab)\n",
    "print(f\"Predicted label 1: {predicted_label}\")\n",
    "predicted_label = predict(model, text2, vocab)\n",
    "print(f\"Predicted label 2: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825642e-a3dd-496a-a9d3-0049ecf192bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04c9d6-9405-46da-a44b-3e9b9b9d1fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
